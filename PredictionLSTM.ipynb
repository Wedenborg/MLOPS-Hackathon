{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "\n",
    "\n",
    "import carbontracker\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import tensorflow as tf\n",
    "import codecarbon\n",
    "from codecarbon import EmissionsTracker\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dda9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tracker\n",
    "tracker = EmissionsTracker(project_name=\"tensorflow_experiment\")\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6be91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/DailyDelhiClimateTrain.csv'\n",
    "df = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a W&B run and set hyperparameters\n",
    "wandb.init(\n",
    "    project=\"MLOPS-Hackathon\",   # Name of your project\n",
    "    entity=\"emiliewedenborg-technical-university-of-denmark\",  # Replace with your W&B entity\n",
    "    name=\"temperature-forecasting-lstm\",  # Name of the run\n",
    "\n",
    "    config={\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"optimizer\": \"adam\"\n",
    "    }\n",
    ")\n",
    "\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01823689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace= True)\n",
    "\n",
    "n_cols = 1\n",
    "dataset = df[\"meantemp\"]\n",
    "dataset = pd.DataFrame(dataset)\n",
    "data = dataset.values\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range= (0, 1))\n",
    "scaled_data = scaler.fit_transform(np.array(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3484d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.75)\n",
    "test_size = len(data) - train_size\n",
    "print(\"Train Size :\",train_size,\"Test Size :\",test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scaled_data[0:train_size, :]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Training set with 60 time-steps\n",
    "x_train = []\n",
    "y_train = []\n",
    "time_steps = 60\n",
    "n_cols = 1\n",
    "\n",
    "for i in range(time_steps, len(scaled_data)):\n",
    "    x_train.append(scaled_data[i-time_steps:i, :n_cols])\n",
    "    y_train.append(scaled_data[i, :n_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the input to (n_samples, time_steps, n_feature)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd69fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(50, return_sequences= True, input_shape= (x_train.shape[1], n_cols)),\n",
    "    LSTM(64, return_sequences= False),\n",
    "    Dense(32),\n",
    "    Dense(16),\n",
    "    Dense(n_cols)\n",
    "])\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'mse' , metrics= [\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb385e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    callbacks=[WandbCallback(save_graph=False)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588def77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"mean_absolute_error\"])\n",
    "plt.legend(['Mean Squared Error','Mean Absolute Error'])\n",
    "plt.title(\"Losses\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop tracker and get emissions\n",
    "emissions = tracker.stop()\n",
    "print(f\"Emissions: {emissions} kg COâ‚‚eq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc23c5e",
   "metadata": {},
   "source": [
    "# Do actual predictions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6beb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a testing set with 60 time-steps and 1 output\n",
    "time_steps = 60\n",
    "test_data = scaled_data[train_size - time_steps:, :]\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "n_cols = 1\n",
    "\n",
    "for i in range(time_steps, len(test_data)):\n",
    "    x_test.append(test_data[i-time_steps:i, 0:n_cols])\n",
    "    y_test.append(test_data[i, 0:n_cols])\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e54f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prediction\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse predictions scaling\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535991bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse y_test scaling\n",
    "y_test = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513effc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean( y_test - predictions )**2).round(2)\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daaf171",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_acts = pd.DataFrame(data={'Predictions':predictions.flatten(), 'Actuals':y_test.flatten()})\n",
    "preds_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e172210",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 6))\n",
    "plt.plot(preds_acts['Predictions'])\n",
    "plt.plot(preds_acts['Actuals'])\n",
    "plt.legend(['Predictions', 'Actuals'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c50d27",
   "metadata": {},
   "source": [
    "# Predict more than just the temp (multivariate predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "cols = list(df.loc[:, ['meantemp', 'humidity', 'wind_speed', 'meanpressure']])\n",
    "dataset = df[cols]\n",
    "dataset = pd.DataFrame(dataset)\n",
    "data = dataset.values\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range= (0, 1))\n",
    "scaled_data = scaler.fit_transform(np.array(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.75)\n",
    "test_size = len(data) - train_size\n",
    "print(\"Train Size :\",train_size,\"Test Size :\",test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scaled_data[0:train_size, :]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5073714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Training set with 60 time-steps\n",
    "x_train = []\n",
    "y_train = []\n",
    "time_steps = 60\n",
    "n_cols = 4\n",
    "\n",
    "for i in range(time_steps, len(train_data)):\n",
    "    x_train.append(train_data[i-time_steps:i, :n_cols])\n",
    "    y_train.append(train_data[i, :n_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the input to (n_samples, time_steps, n_feature)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], n_cols))\n",
    "x_train.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    LSTM(50, return_sequences= True, input_shape= (x_train.shape[1], n_cols)),\n",
    "    LSTM(64, return_sequences= False),\n",
    "    Dense(32),\n",
    "    Dense(16),\n",
    "    Dense(n_cols)\n",
    "])\n",
    "\n",
    "model2.compile(optimizer= 'adam', loss= 'mse' , metrics= [\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be034513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "model2 = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(x_train.shape[1], n_cols)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=64, kernel_size=3, activation=\"relu\"),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(n_cols)\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mean_absolute_error\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    callbacks=[WandbCallback(save_graph=False)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a120fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history2.history[\"loss\"])\n",
    "plt.plot(history2.history[\"mean_absolute_error\"])\n",
    "plt.legend(['Mean Squared Error','Mean Absolute Error'])\n",
    "plt.title(\"Losses\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a testing set with 60 time-steps and 1 output\n",
    "time_steps = 60\n",
    "test_data = scaled_data[train_size - time_steps:, :]\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "n_cols = 4\n",
    "\n",
    "for i in range(time_steps, len(test_data)):\n",
    "    x_test.append(test_data[i-time_steps:i, 0:n_cols])\n",
    "    y_test.append(test_data[i, 0:n_cols])\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prediction\n",
    "predictions = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7025c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse y_test scaling\n",
    "y_test = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ee8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean( y_test - predictions )**2).round(2)\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336777da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_end(Xin, new_input):\n",
    "    timestep = 60\n",
    "    for i in range(timestep - 1):\n",
    "        Xin[:, i, :] = Xin[:, i+1, :]\n",
    "    Xin[:, timestep - 1, :] = new_input\n",
    "    return Xin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = 30\n",
    "forcast = []\n",
    "Xin = x_test[-1 :, :, :]\n",
    "time = []\n",
    "for i in range(0, future):\n",
    "    out = model2.predict(Xin, batch_size=5)\n",
    "    forcast.append(out[0]) \n",
    "    print(forcast)\n",
    "    Xin = insert_end(Xin, out[0, 0]) \n",
    "    time.append(pd.to_datetime(df.index[-1]) + timedelta(days=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ece867",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcasted_output = np.asanyarray(forcast)   \n",
    "forcasted_output = scaler.inverse_transform(forcasted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcasted_output = pd.DataFrame(forcasted_output)\n",
    "date = pd.DataFrame(time)\n",
    "df_result = pd.concat([date,forcasted_output], axis=1)\n",
    "df_result.columns = \"Date\", 'meantemp', 'humidity', 'wind_speed', 'meanpressure'\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Next 30 Days')\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(df['meantemp'])\n",
    "plt.plot(df_result.set_index('Date')[['meantemp']])\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Temp' ,fontsize=18)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(df['humidity'])\n",
    "plt.plot(df_result.set_index('Date')[['humidity']])\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('humidity' ,fontsize=18)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(df['wind_speed'])\n",
    "plt.plot(df_result.set_index('Date')[['wind_speed']])\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('wind_speed' ,fontsize=18)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(df['meanpressure'])\n",
    "plt.plot(df_result.set_index('Date')[['meanpressure']])\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('meanpressure' ,fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"temperature_forecasting_cnn.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b837178",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf_keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfmot\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature_forecasting_cnn.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m prune_low_magnitude \u001b[38;5;241m=\u001b[39m tfmot\u001b[38;5;241m.\u001b[39msparsity\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mprune_low_magnitude\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\__init__.py:86\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# To ensure users only access the expected public API, the API structure is\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# created in the `api` directory. Import all api modules.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\api\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Import API modules for Tensorflow Model Optimization.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\api\\clustering\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Module containing code for clustering.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\api\\clustering\\keras\\__init__.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_scope\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_weights\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m strip_clustering\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\clustering\\keras\\cluster.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_config\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_wrapper\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_centroids\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\clustering\\keras\\cluster_wrapper.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_config\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clusterable_layer\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_centroids\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_registry\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\clustering\\keras\\clustering_centroids.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clustering_ops\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cluster_config\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_model_optimization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m     25\u001b[0m k \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mbackend\n\u001b[0;32m     26\u001b[0m CentroidInitialization \u001b[38;5;241m=\u001b[39m cluster_config\u001b[38;5;241m.\u001b[39mCentroidInitialization\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\compat.py:41\u001b[0m\n\u001b[0;32m     37\u001b[0m     keras_internal \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\n\u001b[0;32m     38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m keras_internal\n\u001b[1;32m---> 41\u001b[0m keras \u001b[38;5;241m=\u001b[39m \u001b[43m_get_keras_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21massign\u001b[39m(ref, value, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tf, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massign\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\askeo\\Documents\\Jobb\\PhD\\02901-AdvancedTopics\\MLOPS-Hackathon\\.venv\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\compat.py:35\u001b[0m, in \u001b[0;36m_get_keras_instance\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m version_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tf\u001b[38;5;241m.\u001b[39mkeras, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_fn \u001b[38;5;129;01mand\u001b[39;00m version_fn()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_internal\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,unused-import\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m   keras_internal \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "model = tf.load(\"temperature_forecasting_cnn.keras\")\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=105)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
    "\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]    # Uncomment this line for Model 2 and Model 3\n",
    "\n",
    "#def representative_data_gen():                          # Uncomment the following 5 lines for Model 3\n",
    "#    for input_value, _ in test_batches.take(200):\n",
    "#        yield [input_value]\n",
    "#converter.representative_dataset = representative_data_gen\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "tflite_models_dir = pathlib.Path(\"\")\n",
    "\n",
    "tflite_model_file = tflite_models_dir/'model1.tflite'     # Change the filename here for Model2 and Model3!\n",
    "x = tflite_model_file.write_bytes(tflite_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecafa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell each time to test your model's accuracy (make sure to change the filename)\n",
    "from tqdm import tqdm\n",
    "from ai_edge_litert.interpreter import Interpreter\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "tflite_model_file = '/content/model1.tflite'                 # Change the filename here for Model 2 and 3\n",
    "interpreter = Interpreter(model_path=tflite_model_file)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "test_labels, test_imgs = [], []\n",
    "for img, label in tqdm(test_batches.take(100)):\n",
    "    interpreter.set_tensor(input_index, img)\n",
    "    interpreter.invoke()\n",
    "    predictions.append(interpreter.get_tensor(output_index))\n",
    "\n",
    "    test_labels.append(label.numpy()[0])\n",
    "    test_imgs.append(img)\n",
    "\n",
    "# For model 1, I got 204.13 it/s\n",
    "# For model 2, I got 156.91 it/s\n",
    "# For model 3, I got 134.71s it/s\n",
    "# Note: since the it/s will depend on the computer on which your Colab VM\n",
    "#       instance is running -- we would expect it to vary a bit.\n",
    "\n",
    "score = 0\n",
    "for item in range(0,100):\n",
    "  prediction=np.argmax(predictions[item])\n",
    "  label = test_labels[item]\n",
    "  if prediction==label:\n",
    "    score=score+1\n",
    "\n",
    "print(\"Out of 100 predictions I got \" + str(score) + \" correct\")\n",
    "\n",
    "# Model 1 - 100 Correct\n",
    "# Model 2 - 99 Correct\n",
    "# Model 3 - 99 Correct\n",
    "# Note: since training starts from a random intialization it would not be\n",
    "#       surprising if your result is off by 1 or 2 correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dbd05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab40b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
